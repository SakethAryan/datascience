{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOtslvDwSRVbKI/JHASW2Py",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chain-veerender/datascience/blob/master/Tree_based_algorithms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Decision Tree**"
      ],
      "metadata": {
        "id": "evo3w75ZC_5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tree structure to show the predictions that result from a series of feature-based splits.\n",
        "\n",
        "It starts with a root node and ends with a decision made by leaves."
      ],
      "metadata": {
        "id": "-6qj6zkyIO2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "![An image](https://editor.analyticsvidhya.com/uploads/498772.png)\n",
        "\n",
        "Image Credentials: https://wiki.pathmind.com/decision-tree "
      ],
      "metadata": {
        "id": "LMEGBy5yJMZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Root Nodes – It is the node present at the beginning of a decision tree from this node the population starts dividing according to various features.\n",
        "\n",
        "Decision Nodes – the nodes we get after splitting the root nodes are called Decision Node\n",
        "\n",
        "Leaf Nodes – the nodes where further splitting is not possible are called leaf nodes or terminal nodes\n",
        "\n",
        "Sub-tree – just like a small portion of a graph is called sub-graph similarly a sub-section of this decision tree is called sub-tree.\n",
        "\n",
        "Pruning – cutting down some nodes to stop overfitting."
      ],
      "metadata": {
        "id": "4fLork3OJyLE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets address following questions:\n",
        "\n",
        "what should be the root node? \n",
        "\n",
        "what should be the decision node? \n",
        "\n",
        "when should I stop splitting? "
      ],
      "metadata": {
        "id": "426FQ7NPKNCm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entropy**\n",
        "\n",
        "Metric to determine the amount of uncertainty in the dataset.\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/706025.png)\n",
        "\n",
        "**It must be as less as possible to accept correpsonding node while parsing a tree.**\n",
        "\n",
        "p+ is the probability of positive class\n",
        "\n",
        "p– is the probability of negative class\n",
        "\n",
        "S is the subset of the training example\n",
        "\n",
        "In order to make a decision tree, we need to calculate the impurity of each split, and when the purity is 100%, we make it as a leaf node/Terminal node.\n",
        "\n",
        "**Goal is to decrease the uncertainty or impurity in the dataset.**\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/618476.png)\n",
        "\n",
        "Image credentials: https://editor.analyticsvidhya.com/uploads/618476.png \n",
        "\n",
        "For feature 2:\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/784167.png)\n",
        "\n",
        "For feature 3:\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/267338.png)\n",
        "\n",
        "How to know if Feature 1 is the choice for root node in above tree ?\n",
        "\n",
        "**Limitation of Entropy:**\n",
        "\n",
        "Using the entropy we are getting the impurity of a particular node, we don’t know if the parent entropy or the entropy of a particular node has decreased or not.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "kE2Wb_RAKq-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Information Gain**\n",
        "\n",
        "Information gain measures the reduction of uncertainty given some feature and it is also a deciding factor for which attribute should be selected as a decision node or root node.\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/410199.png)\n",
        "\n",
        "Eg: Lets say, we have two features to predict whether he/she will go to the gym or not.\n",
        "\n",
        "Feature 1 is “Energy” which takes two values “high” and “low”\n",
        "\n",
        "Feature 2 is “Motivation” which takes 3 values “No motivation”, “Neutral” and “Highly motivated”.\n",
        "\n",
        "Let’s see how our decision tree will be made using these 2 features. We’ll use information gain to decide which feature should be the root node and which feature should be placed after the split.\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/9304810.png)\n",
        "\n",
        "Image credentials: https://editor.analyticsvidhya.com/uploads/9304810.png \n",
        "\n",
        "calculate the entropy:\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/3403611.png)\n",
        "\n",
        "To see the weighted average of entropy of each node:\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/2841712.png)\n",
        "\n",
        "Information gain:\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/4585413.png)\n",
        "\n",
        "Our parent entropy was near 0.99 and after looking at this value of information gain, we can say that the entropy of the dataset will decrease by 0.37 if we make “Energy” as our root node.\n",
        "\n",
        "Similarly, we will do this with the other feature “Motivation” and calculate its information gain.\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/9933814.png)\n",
        "\n",
        "Image credentials: https://editor.analyticsvidhya.com/uploads/9933814.png \n",
        "\n",
        "calculate the entropy here:\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/4554415.png)\n",
        "\n",
        "weighted average of entropy of each node:\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/9594816.png)\n",
        "\n",
        "Information gain:\n",
        "\n",
        "![An image](https://editor.analyticsvidhya.com/uploads/4924117.png)\n",
        "\n",
        "We now see that the “Energy” feature gives more reduction which is 0.37 than the “Motivation” feature.\n",
        "\n",
        "Hence we will select the feature which has the highest information gain and then split the node based on that feature.\n",
        "\n",
        "In this example “Energy” will be our root node and we’ll do the same for sub-nodes. Here we can see that when the energy is “high” the entropy is low and hence we can say a person will definitely go to the gym if he has high energy, but what if the energy is low? We will again split the node based on the new feature which is “Motivation”.\n",
        "\n",
        "**Gini:**\n",
        "\n",
        "![An image](https://lh5.googleusercontent.com/FwI_m1T9nMcYDLw74kvWyeoh3Q8qxeKpp82HcT68kFO7P6TF-AtOsvM8eAyGvKbAI73CqqNOrvB5Wgy3kPbcYtOFplbxmYMI_HCRckVeFbPj79sjclWyrWZzvYgwrIu9FRDU-BLH)\n",
        "\n",
        "Where, \n",
        "\n",
        "The j represents the number of classes in the label, and\n",
        "\n",
        "The P represents the ratio of class at the ith node.\n",
        "\n",
        "![An image](https://lh5.googleusercontent.com/TclqT5gXhhWPiVEu_ZDXkGuT3X5cg7MEd6WM-0xSm7MUqVUD37CKSuwhtr5HFNKZ1TPi9dcCRKLhPhEdL9rQM5P5c5R_qkvxYKKK_ApErbplUpZmTN2DSU2mfpHyhtzoZKgS8W-9)\n",
        "\n",
        "Gini impurity has a maximum value of 0.5, which is the worst we can get, and a minimum value of 0 means the best we can get.\n",
        "\n",
        "\n",
        "**when to stop splitting ?**\n",
        "\n",
        "Usually, real-world datasets have a large number of features, which will result in a large number of splits, which in turn gives a huge tree. Such trees take time to build and can lead to overfitting. That means the tree will give very good accuracy on the training dataset but will give bad accuracy in test data.\n",
        "\n",
        "**Hyperparameter tuning**\n",
        "\n",
        "**max_depth**: set the maximum depth of our decision tree using the **max_depth** parameter. The more the value of max_depth, the more complex your tree will be. The training error will off-course decrease if we increase the max_depth value but when our test data comes into the picture, we will get a very bad accuracy. Hence you need a value that will not overfit as well as underfit our data and for this, you can use GridSearchCV.\n",
        "\n",
        "**min_samples_split**: Another way is to set the minimum number of samples for each spilt. It is denoted by **min_samples_split**. Here we specify the minimum number of samples required to do a spilt. For example, we can use a minimum of 10 samples to reach a decision. That means if a node has less than 10 samples then using this parameter, we can stop the further splitting of this node and make it a leaf node.\n",
        "\n",
        "**min_samples_leaf** – represents the minimum number of samples required to be in the leaf node. The more you increase the number, the more is the possibility of overfitting.\n",
        "\n",
        "**max_features** – it helps us decide what number of features to consider when looking for the best split.\n",
        "\n",
        "**min_weight_fraction_leaf** – Minimum fraction of the sum total of weights required to be at a leaf node.\n",
        "\n",
        "**Pruning**\n",
        "It is another method that can help us avoid overfitting. It helps in improving the performance of the tree by cutting the nodes or sub-nodes which are not significant. It removes the branches which have very low importance.\n",
        "\n",
        "There are mainly 2 ways for pruning:\n",
        "\n",
        "(i) **Pre-pruning** – we can stop growing the tree earlier, which means we can prune/remove/cut a node if it has low importance while growing the tree.\n",
        "\n",
        "(ii) **Post-pruning** – once our tree is built to its depth, we can start pruning the nodes based on their significance.\n",
        "\n"
      ],
      "metadata": {
        "id": "NwG2NLE8bc20"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.analyticsvidhya.com/blog/2021/07/a-comprehensive-guide-to-decision-trees/ \n",
        "\n",
        "https://www.analyticsvidhya.com/blog/2021/08/decision-tree-algorithm/\n",
        "\n"
      ],
      "metadata": {
        "id": "AYyopwaJhh7j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gn3XRW4vCu8S"
      },
      "outputs": [],
      "source": []
    }
  ]
}